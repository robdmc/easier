"""
Tests for the AgentRunner module using real agent calls.

This test suite covers the AgentRunner class functionality including:
- Real agent initialization and calls
- Batch processing with actual responses
- Database integration with real data
- Error handling scenarios
"""

import asyncio
import os
import tempfile
import pytest
from typing import List, Optional, Any
import pandas as pd
import easier as ezr
from pydantic import BaseModel, Field

from easier.agent_runner import AgentRunner, cleanup_all_agents, cancel_all_running_tasks


@pytest.fixture(autouse=True)
def cleanup_before_each_test():
    """Automatically cleanup before each test to ensure clean state"""
    cancel_all_running_tasks()
    yield
    # Cleanup after test as well
    cleanup_all_agents()


@pytest.fixture
def real_agent():
    """Create a real EZAgent for testing with simple system prompt"""
    system_prompt = "You are a helpful assistant. Answer questions briefly in 1-2 words."
    agent = ezr.EZAgent(system_prompt)
    return agent


@pytest.fixture
def temp_db_file():
    """Create a temporary database file path for testing"""
    # Create a temp directory and generate a unique db filename
    import tempfile
    import uuid
    temp_dir = tempfile.gettempdir()
    db_file = os.path.join(temp_dir, f"test_agent_runner_{uuid.uuid4().hex}.db")
    yield db_file
    # Cleanup
    try:
        os.unlink(db_file)
    except FileNotFoundError:
        pass


@pytest.fixture
def simple_prompts():
    """Simple prompts for testing that won't use many tokens"""
    return [
        "What is 1+1?",
        "Color of sky?", 
        "Capital of USA?",
        "Say hello"
    ]


@pytest.fixture
def pydantic_prompts():
    """Prompts that work well with structured Pydantic responses"""
    return [
        "Explain why water boils at 100Â°C. Think through this step by step.",
        "What are the benefits of exercise? Consider multiple aspects.",
        "How does photosynthesis work? Break down the process."
    ]


@pytest.fixture
def simple_framer_func():
    """Simple framer function that converts results to DataFrame"""
    def framer(results: List[Optional[Any]]) -> pd.DataFrame:
        data = []
        for i, result in enumerate(results):
            if result is not None:
                data.append({
                    "prompt_index": i, 
                    "response": str(result.output),
                    "success": True
                })
            else:
                data.append({
                    "prompt_index": i, 
                    "response": "FAILED",
                    "success": False
                })
        return pd.DataFrame(data)
    return framer


@pytest.fixture 
def pydantic_response_model():
    """Pydantic model for structured agent responses"""
    class ResponseModel(BaseModel):
        answer: str = Field(description="The response generated by the agent")
        thoughts: List[str] = Field(description="List of thoughts or reasoning steps")
        
    return ResponseModel


@pytest.fixture
def pydantic_framer_func():
    """Framer function that uses Pydantic model_dump() to create DataFrame rows"""
    def framer(results: List[Optional[Any]]) -> pd.DataFrame:
        data = []
        for i, result in enumerate(results):
            if result is not None and hasattr(result, 'output') and hasattr(result.output, 'model_dump'):
                # Use model_dump() to convert Pydantic model to dict
                row_data = result.output.model_dump()
                row_data["prompt_index"] = i
                row_data["success"] = True
                data.append(row_data)
            else:
                data.append({
                    "prompt_index": i,
                    "answer": "FAILED",
                    "thoughts": [],
                    "success": False
                })
        return pd.DataFrame(data)
    return framer


class TestAgentRunnerInitialization:
    """Test AgentRunner initialization and validation"""
    
    def test_init_with_valid_agent(self, real_agent):
        """Test initialization with valid agent"""
        with AgentRunner(real_agent) as runner:
            assert runner.agent == real_agent
            assert runner.db_enabled is False
            assert runner.db_file is None
            assert runner.table_name == "results"
            assert runner.overwrite is False
            assert runner.timeout == 300.0  # Default timeout
            assert len(runner.active_tasks) == 0
            assert runner._is_running is False
            assert runner._cleanup_done is False
    
    def test_init_with_invalid_agent(self):
        """Test initialization with invalid agent type"""
        with pytest.raises(TypeError, match="Expected agent to be of type ezr.EZAgent"):
            AgentRunner("not an agent")
    
    def test_init_with_database_config(self, real_agent, temp_db_file):
        """Test initialization with database configuration"""
        with AgentRunner(
            real_agent, 
            db_file=temp_db_file, 
            overwrite=True, 
            table_name="test_table",
            timeout=60.0
        ) as runner:
            assert runner.db_enabled is True
            assert runner.db_file == temp_db_file
            assert runner.table_name == "test_table"
            assert runner.overwrite is True
            assert runner.timeout == 60.0
    
    def test_init_database_overwrite(self, real_agent, temp_db_file):
        """Test database initialization with overwrite"""
        # Create the file first
        with open(temp_db_file, 'w') as f:
            f.write("test")
        
        original_exists = os.path.exists(temp_db_file)
        assert original_exists
        
        with AgentRunner(real_agent, db_file=temp_db_file, overwrite=True) as runner:
            # File should have been removed - the AgentRunner only deletes it,
            # doesn't recreate until first write
            assert not os.path.exists(temp_db_file)
    
    def test_context_manager_cleanup(self, real_agent):
        """Test that context manager properly cleans up"""
        runner = AgentRunner(real_agent)
        
        with runner:
            assert runner._cleanup_done is False
        
        # After exiting context, cleanup should be done
        assert runner._cleanup_done is True


class TestBatchCreation:
    """Test batch creation functionality"""
    
    def test_create_batches_exact_division(self, real_agent):
        """Test batch creation when prompts divide evenly"""
        with AgentRunner(real_agent) as runner:
            prompts = ["What is 1+1?", "Color of sky?", "Capital of USA?", "Say hello"]
            batches = list(runner.create_batches(prompts, batch_size=2))
            
            assert len(batches) == 2
            assert batches[0] == ["What is 1+1?", "Color of sky?"]
            assert batches[1] == ["Capital of USA?", "Say hello"]
    
    def test_create_batches_remainder(self, real_agent, simple_prompts):
        """Test batch creation with remainder"""
        with AgentRunner(real_agent) as runner:
            # Add one more prompt to create remainder
            prompts = simple_prompts + ["Extra prompt"]
            batches = list(runner.create_batches(prompts, batch_size=2))
            
            assert len(batches) == 3
            assert batches[0] == ["What is 1+1?", "Color of sky?"]
            assert batches[1] == ["Capital of USA?", "Say hello"]
            assert batches[2] == ["Extra prompt"]
    
    def test_create_batches_empty_list(self, real_agent):
        """Test batch creation with empty prompt list"""
        with AgentRunner(real_agent) as runner:
            batches = list(runner.create_batches([], batch_size=2))
            assert len(batches) == 0


class TestProcessBatch:
    """Test single batch processing"""
    
    @pytest.mark.asyncio
    async def test_process_batch_success(self, real_agent):
        """Test successful batch processing"""
        with AgentRunner(real_agent) as runner:
            prompts = ["What is 1+1?", "Color of sky?"]
            results = await runner.process_batch(prompts)
            
            assert len(results) == 2
            # Check that we got actual results (not None)
            assert results[0] is not None
            assert results[1] is not None
            # Results should have an output attribute
            assert hasattr(results[0], 'output')
            assert hasattr(results[1], 'output')
    
    @pytest.mark.asyncio
    async def test_process_batch_with_framer(self, real_agent, simple_framer_func):
        """Test batch processing with framer function"""
        with AgentRunner(real_agent) as runner:
            prompts = ["What is 1+1?", "Color of sky?"]
            result = await runner.process_batch(prompts, framer_func=simple_framer_func)
            
            assert isinstance(result, pd.DataFrame)
            assert len(result) == 2
            assert "prompt_index" in result.columns
            assert "response" in result.columns
            assert "success" in result.columns
            # Both should be successful
            assert all(result["success"])
    
    @pytest.mark.asyncio
    async def test_process_batch_with_output_type(self, real_agent):
        """Test batch processing with output type specification"""
        with AgentRunner(real_agent) as runner:
            prompts = ["What is 1+1?"]
            results = await runner.process_batch(prompts, output_type=str)
            
            assert len(results) == 1
            assert results[0] is not None
            # Should still get a result with output
            assert hasattr(results[0], 'output')
    
    @pytest.mark.asyncio
    async def test_process_batch_with_timeout(self, real_agent):
        """Test batch processing respects timeout setting"""
        with AgentRunner(real_agent, timeout=0.001) as runner:  # Very short timeout
            prompts = ["What is 1+1?"]
            results = await runner.process_batch(prompts)
            
            # With such a short timeout, we expect the request to timeout and return None
            assert len(results) == 1
            assert results[0] is None  # Should timeout and return None


class TestDatabaseIntegration:
    """Test database integration functionality"""
    
    @pytest.mark.asyncio
    async def test_write_batch_to_db(self, real_agent, temp_db_file):
        """Test writing batch results to database"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            # Create test DataFrame
            df = pd.DataFrame({"col1": [1, 2], "col2": ["a", "b"]})
            semaphore = asyncio.Semaphore(1)
            
            # Should not raise an error
            await runner._write_batch_to_db(df, semaphore)
        
        # Verify data was written by reading it back
        import duckdb
        con = duckdb.connect(temp_db_file)
        try:
            result = con.execute("SELECT * FROM results").fetchall()
            assert len(result) == 2
            assert result[0] == (1, "a")
            assert result[1] == (2, "b")
        finally:
            con.close()
    
    @pytest.mark.asyncio
    async def test_write_batch_empty_dataframe(self, real_agent, temp_db_file):
        """Test writing empty DataFrame to database"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            empty_df = pd.DataFrame()
            semaphore = asyncio.Semaphore(1)
            
            # Should not raise an error and should return early
            await runner._write_batch_to_db(empty_df, semaphore)
        
        # Verify no table was created
        import duckdb
        con = duckdb.connect(temp_db_file)
        try:
            tables = con.execute("SHOW TABLES").fetchall()
            assert len(tables) == 0
        except:
            # If SHOW TABLES fails, that's also fine - no data was written
            pass
        finally:
            con.close()


class TestPydanticModelIntegration:
    """Test Pydantic model integration with and without framer functions"""
    
    @pytest.mark.asyncio
    async def test_pydantic_model_without_framer(self, real_agent, pydantic_response_model, pydantic_prompts):
        """Test using Pydantic model without framer function"""
        with AgentRunner(real_agent) as runner:
            # Use a single prompt to keep test simple and fast
            test_prompt = pydantic_prompts[0]
            results = await runner.run([test_prompt], output_type=pydantic_response_model)
        
        assert len(results) == 1
        assert results[0] is not None
        assert hasattr(results[0], 'output')
        
        # Verify the output is a Pydantic model instance
        output = results[0].output
        assert hasattr(output, 'model_dump')
        assert hasattr(output, 'answer')
        assert hasattr(output, 'thoughts')
        
        # Verify the structure
        model_dict = output.model_dump()
        assert 'answer' in model_dict
        assert 'thoughts' in model_dict
        assert isinstance(model_dict['answer'], str)
        assert isinstance(model_dict['thoughts'], list)
    
    @pytest.mark.asyncio
    async def test_pydantic_model_with_framer(self, real_agent, pydantic_response_model, pydantic_framer_func, pydantic_prompts):
        """Test using Pydantic model with framer function that uses model_dump()"""
        with AgentRunner(real_agent) as runner:
            # Use first two prompts to test batch processing
            test_prompts = pydantic_prompts[:2]
            result = await runner.run(
                test_prompts, 
                output_type=pydantic_response_model,
                framer_func=pydantic_framer_func,
                batch_size=2
            )
        
        assert isinstance(result, pd.DataFrame)
        assert len(result) == 2
        
        # Verify DataFrame has expected columns from model_dump()
        expected_columns = {'prompt_index', 'success', 'answer', 'thoughts'}
        assert expected_columns.issubset(set(result.columns))
        
        # Verify all were successful
        assert all(result["success"])
        
        # Verify data types and content
        assert all(isinstance(answer, str) for answer in result["answer"])
        assert all(isinstance(thoughts, list) for thoughts in result["thoughts"])
        assert all(len(answer) > 0 for answer in result["answer"])
    
    @pytest.mark.asyncio
    async def test_pydantic_model_with_database_integration(self, real_agent, pydantic_response_model, pydantic_framer_func, pydantic_prompts, temp_db_file):
        """Test Pydantic model with database persistence using model_dump()"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            # Use single prompt for faster test
            test_prompt = pydantic_prompts[0]
            result = await runner.run(
                [test_prompt],
                output_type=pydantic_response_model,
                framer_func=pydantic_framer_func
            )
        
        assert isinstance(result, pd.DataFrame)
        assert len(result) == 1
        
        # Verify database persistence
        import duckdb
        con = duckdb.connect(temp_db_file)
        try:
            stored_results = con.execute("SELECT * FROM results").fetchall()
            assert len(stored_results) == 1
            
            # Verify we have the structured data from model_dump()
            row = stored_results[0]
            # The exact column order depends on DataFrame column order, but we should have:
            # prompt_index, success, answer, thoughts (in some order)
            assert len(row) >= 4  # At least these 4 columns
            
            # Find the success column (should be True)
            success_found = False
            answer_found = False
            for value in row:
                if value is True:  # success column
                    success_found = True
                elif isinstance(value, str) and len(value) > 0 and value != "FAILED":
                    answer_found = True
            
            assert success_found, "Success column not found or not True"
            assert answer_found, "Answer column not found or empty"
            
        finally:
            con.close()


class TestRunMethod:
    """Test the main run method with various scenarios"""
    
    @pytest.mark.asyncio
    async def test_run_basic(self, real_agent, simple_prompts):
        """Test basic run functionality"""
        with AgentRunner(real_agent) as runner:
            results = await runner.run(simple_prompts, batch_size=2, max_concurrency=2)
        
        assert len(results) == len(simple_prompts)
        # All results should be valid (not None)
        assert all(result is not None for result in results)
        # All results should have output
        assert all(hasattr(result, 'output') for result in results)
    
    @pytest.mark.asyncio
    async def test_run_with_framer(self, real_agent, simple_prompts, simple_framer_func):
        """Test run with framer function"""
        with AgentRunner(real_agent) as runner:
            result = await runner.run(
                simple_prompts, 
                batch_size=2, 
                max_concurrency=2, 
                framer_func=simple_framer_func
            )
        
        assert isinstance(result, pd.DataFrame)
        assert len(result) == len(simple_prompts)
        assert "prompt_index" in result.columns
        assert "response" in result.columns
        assert "success" in result.columns
        # All should be successful
        assert all(result["success"])
    
    @pytest.mark.asyncio
    async def test_run_database_without_framer_error(self, real_agent, simple_prompts, temp_db_file):
        """Test that database mode requires framer function"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            with pytest.raises(ValueError, match="Database file .* is configured but framer_func is not provided"):
                await runner.run(simple_prompts)
    
    @pytest.mark.asyncio
    async def test_run_with_database(self, real_agent, simple_prompts, simple_framer_func, temp_db_file):
        """Test run with database persistence"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            result = await runner.run(
                simple_prompts,
                batch_size=2,
                max_concurrency=2,
                framer_func=simple_framer_func
            )
        
        assert isinstance(result, pd.DataFrame)
        assert len(result) == len(simple_prompts)
        
        # Verify data was persisted to database
        import duckdb
        con = duckdb.connect(temp_db_file)
        try:
            stored_results = con.execute("SELECT * FROM results").fetchall()
            assert len(stored_results) == len(simple_prompts)
        finally:
            con.close()
    
    @pytest.mark.asyncio
    async def test_run_empty_prompts(self, real_agent):
        """Test run with empty prompt list"""
        with AgentRunner(real_agent) as runner:
            results = await runner.run([])
        
        assert len(results) == 0
    
    @pytest.mark.asyncio
    async def test_run_single_prompt(self, real_agent):
        """Test run with single prompt"""
        with AgentRunner(real_agent) as runner:
            results = await runner.run(["What is 1+1?"])
        
        assert len(results) == 1
        assert results[0] is not None
        assert hasattr(results[0], 'output')


class TestErrorHandling:
    """Test error handling and edge cases"""
    
    @pytest.mark.asyncio
    async def test_run_with_invalid_prompts(self, real_agent):
        """Test handling when prompts might cause issues"""
        with AgentRunner(real_agent) as runner:
            # Test with various edge case prompts
            edge_prompts = [
                "",  # Empty string
                "What is 1+1?",  # Normal prompt
                "A" * 50,  # Long prompt (but not too long)
            ]
            
            results = await runner.run(edge_prompts, batch_size=2)
        
        # Should handle all prompts gracefully
        assert len(results) == len(edge_prompts)
        # At least the normal prompt should work
        assert any(result is not None for result in results)


class TestAdvancedErrorHandling:
    """Test advanced error handling scenarios and edge cases"""
    
    @pytest.mark.asyncio
    async def test_agent_run_failure_handling(self, real_agent, simple_framer_func):
        """Test handling when individual agent.run calls fail"""
        with AgentRunner(real_agent) as runner:
            # Create a mock agent that will fail on certain prompts
            import unittest.mock
            
            original_run = runner.agent.run
            
            async def failing_run(prompt, output_type=None):
                if "FAIL_THIS_PROMPT" in prompt:
                    raise Exception("Simulated agent failure")
                return await original_run(prompt, output_type=output_type)
            
            # Mock the agent's run method
            with unittest.mock.patch.object(runner.agent, 'run', side_effect=failing_run):
                prompts = ["What is 1+1?", "FAIL_THIS_PROMPT", "What is 2+2?"]
                
                # Test without framer (should return list with None for failed prompt)
                results = await runner.run(prompts, batch_size=2)
                
                assert len(results) == 3
                assert results[0] is not None  # First prompt should succeed
                assert results[1] is None      # Second prompt should fail (None)
                assert results[2] is not None  # Third prompt should succeed
                
                # Test with framer (should handle None results gracefully)
                result_df = await runner.run(prompts, batch_size=2, framer_func=simple_framer_func)
                
                assert isinstance(result_df, pd.DataFrame)
                assert len(result_df) == 3
                assert result_df.iloc[0]["success"] == True   # First should succeed
                assert result_df.iloc[1]["success"] == False  # Second should fail
                assert result_df.iloc[2]["success"] == True   # Third should succeed
                assert result_df.iloc[1]["response"] == "FAILED"  # Failed response
    
    @pytest.mark.asyncio 
    async def test_database_connection_failure(self, real_agent, simple_framer_func, temp_db_file):
        """Test handling when database connection fails"""
        with AgentRunner(real_agent, db_file="/invalid/path/database.db") as runner:
            # The AgentRunner handles database failures gracefully and returns empty DataFrame
            result = await runner.run(
                ["What is 1+1?"], 
                framer_func=simple_framer_func
            )
        
        # Should return empty DataFrame when all batches fail due to DB issues
        assert isinstance(result, pd.DataFrame)
        assert len(result) == 0
    
    @pytest.mark.asyncio
    async def test_framer_function_failure(self, real_agent):
        """Test handling when framer function fails"""
        with AgentRunner(real_agent) as runner:
            def failing_framer(results):
                raise Exception("Framer function failed")
            
            # The AgentRunner handles framer failures and returns empty DataFrame
            result = await runner.run(
                ["What is 1+1?"], 
                framer_func=failing_framer
            )
        
        # Should return empty DataFrame when framer fails
        assert isinstance(result, pd.DataFrame)
        assert len(result) == 0
    
    @pytest.mark.asyncio
    async def test_empty_dataframe_return_on_all_failures(self, real_agent, simple_framer_func):
        """Test that empty DataFrame is returned when all batches fail with framer"""
        with AgentRunner(real_agent) as runner:
            # Create a framer that always returns None (simulating all batch failures)
            def always_fail_framer(results):
                return simple_framer_func(results)
            
            # Mock the process_batch method to always return None (simulating batch failures)
            import unittest.mock
            
            original_process_batch = runner.process_batch
            
            async def failing_process_batch(*args, **kwargs):
                # Simulate a scenario where process_batch returns None due to failure
                return None
            
            with unittest.mock.patch.object(runner, 'process_batch', side_effect=failing_process_batch):
                result = await runner.run(
                    ["What is 1+1?", "What is 2+2?"], 
                    batch_size=1,
                    framer_func=simple_framer_func
                )
                
                # Should return empty DataFrame when all batches fail
                assert isinstance(result, pd.DataFrame)
                assert len(result) == 0
    
    @pytest.mark.asyncio
    async def test_database_write_with_corrupted_data(self, real_agent, temp_db_file):
        """Test database handling with corrupted/invalid DataFrame"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            def corrupted_framer(results):
                # Return a DataFrame with problematic data that might cause DB issues
                import pandas as pd
                import numpy as np
                
                return pd.DataFrame({
                    "col1": [float('inf'), float('-inf'), np.nan],  # Problematic float values
                    "col2": [None, None, None],  # All None values
                    "col3": ["test", "test2", "test3"]  # Normal strings
                })
            
            # This might fail during database write due to problematic data
            # The exact behavior depends on DuckDB's handling of inf/nan values
            try:
                result = await runner.run(
                    ["What is 1+1?"], 
                    framer_func=corrupted_framer
                )
                # If it succeeds, verify the result
                assert isinstance(result, pd.DataFrame)
            except Exception:
                # If it fails, that's also acceptable for this test
                # The important thing is that we don't crash the whole system
                pass
    
    @pytest.mark.asyncio
    async def test_database_transaction_rollback(self, real_agent, temp_db_file):
        """Test database rollback handling when transaction fails"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            def problematic_framer(results):
                # Create a DataFrame with a column name that will cause issues
                import pandas as pd
                return pd.DataFrame({
                    "normal_col": ["test"],
                    "SELECT * FROM": ["malicious"],  # Column name that might cause SQL issues
                })
            
            # Mock duckdb connection to simulate transaction failure
            import unittest.mock
            import duckdb
            
            def mock_connect(db_file):
                mock_con = unittest.mock.Mock()
                mock_con.begin = unittest.mock.Mock()
                mock_con.execute = unittest.mock.Mock(side_effect=Exception("SQL execution failed"))
                mock_con.rollback = unittest.mock.Mock()
                mock_con.commit = unittest.mock.Mock()
                mock_con.close = unittest.mock.Mock()
                return mock_con
            
            with unittest.mock.patch('duckdb.connect', side_effect=mock_connect):
                # This should trigger the rollback error handling path and return empty DataFrame
                result = await runner.run(
                    ["What is 1+1?"], 
                    framer_func=problematic_framer
                )
                
                # Should return empty DataFrame when database operations fail
                assert isinstance(result, pd.DataFrame)
                assert len(result) == 0
    
    @pytest.mark.asyncio
    async def test_database_rollback_also_fails(self, real_agent, temp_db_file):
        """Test handling when both database operation and rollback fail"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            def simple_framer(results):
                import pandas as pd
                return pd.DataFrame({"col": ["test"]})
            
            # Mock duckdb connection where both execute and rollback fail
            import unittest.mock
            import duckdb
            
            def mock_connect(db_file):
                mock_con = unittest.mock.Mock()
                mock_con.begin = unittest.mock.Mock()
                mock_con.execute = unittest.mock.Mock(side_effect=Exception("SQL execution failed"))
                mock_con.rollback = unittest.mock.Mock(side_effect=Exception("Rollback failed"))
                mock_con.commit = unittest.mock.Mock()
                mock_con.close = unittest.mock.Mock()
                return mock_con
            
            with unittest.mock.patch('duckdb.connect', side_effect=mock_connect):
                # This should handle the case where rollback also fails and return empty DataFrame
                result = await runner.run(
                    ["What is 1+1?"], 
                    framer_func=simple_framer
                )
                
                # Should return empty DataFrame when both operations fail
                assert isinstance(result, pd.DataFrame)
                assert len(result) == 0


class TestConcurrencyControl:
    """Test concurrency control mechanisms"""
    
    @pytest.mark.asyncio
    async def test_batching_behavior(self, real_agent, simple_prompts):
        """Test that batching works correctly with real agents"""
        with AgentRunner(real_agent) as runner:
            # Test with different batch sizes
            result1 = await runner.run(simple_prompts, batch_size=1, max_concurrency=2)
            result2 = await runner.run(simple_prompts, batch_size=4, max_concurrency=1)
        
        # Both should return the same number of results
        assert len(result1) == len(result2) == len(simple_prompts)
        # All results should be valid
        assert all(r is not None for r in result1)
        assert all(r is not None for r in result2)


@pytest.mark.integration 
class TestIntegration:
    """Integration tests that test the full workflow"""
    
    @pytest.mark.asyncio
    async def test_full_workflow_with_database(self, real_agent, simple_prompts, simple_framer_func, temp_db_file):
        """Test complete workflow with database persistence"""
        with AgentRunner(real_agent, db_file=temp_db_file) as runner:
            result = await runner.run(
                simple_prompts,
                batch_size=2,
                max_concurrency=2,
                framer_func=simple_framer_func
            )
        
        # Verify results
        assert isinstance(result, pd.DataFrame)
        assert len(result) == len(simple_prompts)
        assert "prompt_index" in result.columns
        assert "response" in result.columns
        assert "success" in result.columns
        
        # Verify database persistence
        import duckdb
        con = duckdb.connect(temp_db_file)
        try:
            stored_results = con.execute("SELECT * FROM results").fetchall()
            # Should have same number of results as prompts
            assert len(stored_results) == len(simple_prompts)
            
            # Verify all were successful (success is the 3rd column, index 2)
            assert all(row[2] == True for row in stored_results)  # success column
            
            # Verify we have response data (response is the 2nd column, index 1)
            assert all(row[1] and row[1] != "FAILED" for row in stored_results)  # response column
        finally:
            con.close()
    
    @pytest.mark.asyncio
    async def test_workflow_without_database(self, real_agent, simple_prompts, simple_framer_func):
        """Test complete workflow without database"""
        with AgentRunner(real_agent) as runner:
            result = await runner.run(
                simple_prompts,
                batch_size=2,
                max_concurrency=2,
                framer_func=simple_framer_func
            )
        
        # Verify results
        assert isinstance(result, pd.DataFrame)
        assert len(result) == len(simple_prompts)
        assert all(result["success"])


@pytest.mark.integration 
class TestCleanupAndTimeoutFeatures:
    """Test new cleanup and timeout functionality"""
    
    @pytest.mark.asyncio
    async def test_manual_cleanup_functions(self, real_agent):
        """Test manual cleanup functions work correctly"""
        # Create runner but don't use context manager
        runner = AgentRunner(real_agent)
        
        # Simulate some running state
        runner._is_running = True
        
        # Test manual cleanup
        cancel_all_running_tasks()
        cleanup_all_agents()
        
        # Verify cleanup was called
        assert runner._cleanup_done is True
    
    @pytest.mark.asyncio
    async def test_timeout_configuration(self, real_agent):
        """Test timeout configuration is properly set"""
        with AgentRunner(real_agent, timeout=120.0) as runner:
            assert runner.timeout == 120.0
    
    @pytest.mark.asyncio
    async def test_task_tracking(self, real_agent, simple_prompts):
        """Test that tasks are properly tracked"""
        with AgentRunner(real_agent) as runner:
            # Initially no tasks
            assert len(runner.active_tasks) == 0
            
            # Start a run (but don't await it immediately)
            task = asyncio.create_task(runner.run(simple_prompts[:1], batch_size=1))
            
            # Give it a moment to start
            await asyncio.sleep(0.1)
            
            # Should have active tasks while running
            # (This test is somewhat timing-dependent)
            
            # Complete the task
            await task
    
    @pytest.mark.asyncio
    async def test_context_manager_cleanup_on_exception(self, real_agent):
        """Test context manager cleans up even when exception occurs"""
        runner = AgentRunner(real_agent)
        
        try:
            with runner:
                # Simulate some work
                runner._is_running = True
                # Raise an exception
                raise ValueError("Test exception")
        except ValueError:
            pass  # Expected
        
        # Cleanup should still have been called
        assert runner._cleanup_done is True
    
    @pytest.mark.asyncio
    async def test_timeout_prevents_hanging(self, real_agent):
        """Test that timeout prevents requests from hanging indefinitely"""
        # Use a very short timeout to test timeout behavior
        with AgentRunner(real_agent, timeout=0.001) as runner:
            # This should timeout quickly
            results = await runner.run(["Tell me a long story"], batch_size=1)
            
            # Should return None due to timeout
            assert len(results) == 1
            assert results[0] is None
    
    def test_global_task_tracker_singleton(self):
        """Test that TaskTracker is a proper singleton"""
        from easier.agent_runner import _task_tracker, TaskTracker
        
        # Create new instances
        tracker1 = TaskTracker()
        tracker2 = TaskTracker()
        
        # Should be the same instance
        assert tracker1 is tracker2
        assert tracker1 is _task_tracker
    
    @pytest.mark.asyncio
    async def test_multiple_runners_cleanup(self, real_agent):
        """Test cleanup works with multiple runner instances"""
        runners = []
        
        try:
            # Create multiple runners
            for i in range(3):
                runner = AgentRunner(real_agent, timeout=30.0)
                runners.append(runner)
                runner._is_running = True
            
            # Global cleanup should clean up all
            cleanup_all_agents()
            
            # All should be cleaned up
            for runner in runners:
                assert runner._cleanup_done is True
                
        finally:
            # Ensure cleanup
            for runner in runners:
                try:
                    runner.cleanup()
                except:
                    pass